from flask import Flask, request, jsonify
import asyncio
import json
import re
import traceback
import logging
from jd import JDCommentScraper
import mysql.connector
from flask_socketio import SocketIO
import threading
from datetime import datetime
from flask_cors import CORS
from pathlib import Path

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('jd_crawler')

app = Flask(__name__, static_folder='CommentAnalysor_frontend/vue/dist', static_url_path='')
# 启用CORS跨域资源共享
CORS(app, resources={r"/*": {"origins": "*"}})
app.config['SECRET_KEY'] = 'jd_crawler_secret'
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading', logger=True, engineio_logger=True)

# 数据库配置
db_config = {
    "host": "localhost",
    "user": "root",
    "password": "12345678",
    "database": "SEP"
}

# 创建爬虫类的扩展，增加实时消息推送功能
class WebSocketJDScraper(JDCommentScraper):
    def __init__(self, product_id, product_name, headless=True, test_mode=True):
        # 确保有一个独立的user_data_dir路径
        user_data_dir = Path(__file__).parent / "jd_user_data" / f"profile_{product_id}"
        user_data_dir.mkdir(parents=True, exist_ok=True)
        super().__init__(headless=headless, test_mode=test_mode, user_data_dir=str(user_data_dir))
        self.product_id = product_id
        self.product_name = product_name
        self.total_comments_count = 0
    
    # 重写拦截评论方法，添加实时推送
    async def intercept_comments(self, route, request):
        try:
            await route.continue_()
            response = await request.response()
            
            if response and response.ok:
                try:
                    body = await response.text()
                    
                    # 处理JSONP响应
                    if 'fetchJSON_comment' in body:
                        json_str = re.search(r'fetchJSON_comment\d*\((.*)\);', body)
                        if json_str:
                            body = json_str.group(1)
                    
                    data = json.loads(body)
                    
                    if 'comments' in data:
                        comments = data['comments']
                        self.total_comments_count += len(comments)
                        logger.info(f"已爬取 {self.total_comments_count} 条评论")
                        socketio.emit('progress', {'status': 'crawling', 'count': self.total_comments_count})
                        
                        for comment in comments:
                            if comment.get('content'):  # 只添加有内容的评论
                                comment_data = {
                                    'content': comment.get('content', ''),
                                    'creationTime': comment.get('creationTime', ''),
                                    'nickname': comment.get('nickname', ''),
                                    'score': comment.get('score', 0),
                                    'userLevelName': comment.get('userLevelName', ''),
                                    'productColor': comment.get('productColor', ''),
                                    'productSize': comment.get('productSize', ''),
                                    'images': comment.get('images', []),
                                    'product_id': self.product_id,
                                    'product_name': self.product_name
                                }
                                
                                # 实时推送新评论
                                socketio.emit('new_comment', comment_data)
                                
                                # 避免重复添加相同评论
                                content_exists = any(c['content'] == comment_data['content'] and 
                                                   c['nickname'] == comment_data['nickname'] 
                                                   for c in self.captured_comments)
                                if not content_exists:
                                    self.captured_comments.append(comment_data)
                                    # 立即保存到数据库
                                    save_comment_to_db(comment_data)
                except Exception as e:
                    logger.error(f"处理拦截的评论数据时出错: {e}")
                    logger.error(traceback.format_exc())
                    socketio.emit('error', {'message': f'处理评论数据错误: {str(e)}'})
        except Exception as e:
            logger.error(f"拦截评论请求失败: {e}")
            logger.error(traceback.format_exc())
            socketio.emit('error', {'message': f'拦截评论请求失败: {str(e)}'})

# 保存评论到数据库
def save_comment_to_db(comment_data):
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        # 检查产品是否存在，不存在则插入
        check_product_sql = "SELECT id FROM product WHERE id = %s"
        cursor.execute(check_product_sql, (comment_data['product_id'],))
        product_exists = cursor.fetchone()
        
        if not product_exists:
            # 插入产品信息
            insert_product_sql = """INSERT INTO product 
                                  (id, name, created_time, updated_time) 
                                  VALUES (%s, %s, NOW(), NOW())"""
            cursor.execute(insert_product_sql, (
                comment_data['product_id'],
                comment_data['product_name']
            ))
            conn.commit()
            logger.info(f"商品已保存到数据库: {comment_data['product_id']} - {comment_data['product_name']}")
        
        # 检查评论表是否存在，不存在则创建
        table_name = f"comment_{comment_data['product_id']}"
        check_table_sql = f"SHOW TABLES LIKE '{table_name}'"
        cursor.execute(check_table_sql)
        table_exists = cursor.fetchone()
        
        if not table_exists:
            # 创建评论表
            create_table_sql = f"""
            CREATE TABLE `{table_name}` (
              `id` int NOT NULL AUTO_INCREMENT,
              `content` text CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
              `nickname` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL,
              `score` int DEFAULT NULL,
              `create_time` datetime DEFAULT NULL,
              `category` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL,
              PRIMARY KEY (`id`)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
            """
            cursor.execute(create_table_sql)
            conn.commit()
            logger.info(f"评论表已创建: {table_name}")
        
        # 转换日期格式
        creation_time = datetime.strptime(comment_data['creationTime'], 
                                         '%Y-%m-%d %H:%M:%S')
        
        # 插入评论数据
        insert_comment_sql = f"""INSERT INTO {table_name} 
                               (content, nickname, score, create_time) 
                               VALUES (%s, %s, %s, %s)"""
        
        cursor.execute(insert_comment_sql, (
            comment_data['content'],
            comment_data['nickname'],
            comment_data['score'],
            creation_time
        ))
        
        conn.commit()
        logger.info(f"评论已保存到数据库: {comment_data['nickname']}")
        conn.close()
        
    except Exception as e:
        logger.error(f"保存评论到数据库失败: {e}")
        logger.error(traceback.format_exc())
        socketio.emit('error', {'message': f'保存评论失败: {str(e)}'})

# 检查数据库连接
def check_database_connection():
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("SELECT 1")
        result = cursor.fetchone()
        conn.close()
        return True if result else False
    except Exception as e:
        logger.error(f"数据库连接测试失败: {e}")
        return False

# 后台执行爬虫任务
def run_crawler(product_url, product_id, product_name):
    # 关闭测试模式，爬取真实评论
    always_test_mode = False
    
    async def run():
        scraper = None
        try:
            logger.info(f"开始爬取商品: {product_id} - {product_name}")
            socketio.emit('progress', {'status': 'starting', 'product_id': product_id})
            
            # 初始化爬虫实例
            scraper = WebSocketJDScraper(product_id, product_name, headless=True, test_mode=always_test_mode)
            # 设置浏览器
            await scraper.setup()
            
            # 开始爬取评论
            await scraper.load_comments(product_url)
            
            logger.info(f"商品 {product_id} 爬取完成，共获取 {len(scraper.captured_comments)} 条评论")
            socketio.emit('progress', {
                'status': 'completed', 
                'count': len(scraper.captured_comments),
                'product_id': product_id
            })
        except Exception as e:
            logger.error(f"爬虫执行错误: {e}")
            logger.error(traceback.format_exc())
            socketio.emit('error', {'message': f'爬虫执行错误: {str(e)}'})
        finally:
            if scraper:
                try:
                    await scraper.close()
                except Exception as e:
                    logger.error(f"关闭爬虫时出错: {e}")
    
    # 在事件循环中运行异步任务
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(run())

@app.route('/api/crawl', methods=['POST'])
def start_crawl():
    try:
        data = request.json
        logger.info(f"收到爬取请求: {data}")
        
        product_url = data.get('url')
        product_id = data.get('product_id')
        product_name = data.get('product_name', '')
        
        if not product_url or not product_id:
            logger.warning("请求缺少必要参数")
            return jsonify({'success': False, 'message': '缺少商品URL或ID'}), 400
        
        # 检查数据库连接
        if not check_database_connection():
            return jsonify({'success': False, 'message': '数据库连接失败，请检查配置'}), 500
        
        # 在后台线程中执行爬虫
        thread = threading.Thread(target=run_crawler, args=(product_url, product_id, product_name))
        thread.daemon = True
        thread.start()
        
        return jsonify({'success': True, 'message': '爬虫已启动'})
    except Exception as e:
        logger.error(f"启动爬虫失败: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'message': f'启动爬虫失败: {str(e)}'}), 500

@socketio.on('connect')
def handle_connect():
    logger.info(f"客户端已连接: {request.sid}")

@socketio.on('disconnect')
def handle_disconnect():
    logger.info(f"客户端已断开连接: {request.sid}")

@app.route('/')
def index():
    return app.send_static_file('index.html')

@app.route('/api/status')
def status():
    return jsonify({"status": "服务正常运行", "version": "1.0"})

if __name__ == '__main__':
    # 启动前检查数据库连接
    if check_database_connection():
        logger.info("数据库连接正常")
    else:
        logger.error("数据库连接失败，服务可能无法正常工作")
        
    logger.info("启动Flask-SocketIO服务，监听端口 5004")
    socketio.run(app, host='0.0.0.0', port=5004, debug=False, allow_unsafe_werkzeug=True)
